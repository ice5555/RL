### 2. 先写后续代码





- 继续补全剩下的 RL 流水线模块（比如 Action space 扩展、奖励函数改进、DPO/GRPO 等）。
- 好处：先让功能全部可跑，画图留到最后一次性处理。
- 风险：如果日志记录没提前验证，最后才发现格式不合预期，会回溯改一堆代码









好，我帮你画一个流水线级 RL（Pipeline-RL）架构 + 实验对比设计的图，结构跟你论文的实验部分可以直接挂钩。



------





### Pipeline-RL 架构图



```
            ┌──────────────────────┐
            │      用户查询 Q      │
            └─────────┬────────────┘
                      │
               (1) Query 改写/增强  ←─ [可选动作: 改写次数/策略]
                      │
            ┌─────────▼────────────┐
            │      检索 Retriever   │
            │  BM25 / Dense / Hybrid│
            └─────────┬────────────┘
                      │
         [动作: top_k, 稀疏/稠密权重, rerank 开关]
                      │
            ┌─────────▼────────────┐
            │      生成器 LLM       │
            │   Flan-T5 / GPT 等    │
            └─────────┬────────────┘
                      │
             [动作: temperature, max_tokens]
                      │
            ┌─────────▼────────────┐
            │       答案输出        │
            └─────────┬────────────┘
                      │
               (2) 奖励计算 Reward
         ┌────────────┼──────────────┐
         │   指标类: F1, EM          │
         │   效率类: 延迟, Token数   │
         └──────────────────────────┘
                      │
            ┌─────────▼────────────┐
            │       RL Agent        │
            │ PPO / DQN / GRPO ...  │
            └──────────────────────┘
                      │
             动作策略更新 Policy Update
```



------





### 实验设计对比表



| 实验编号 | 方法                    | Action Space        | RL算法 | Reward            | 对照组 | 目的           |
| -------- | ----------------------- | ------------------- | ------ | ----------------- | ------ | -------------- |
| B1       | Baseline 固定参数       | -                   | -      | -                 | -      | 作为起点       |
| R1       | Pipeline-RL(Top_k)      | top_k ∈ {3,5,10}    | PPO    | F1                | B1     | 验证 RL 优势   |
| R2       | Pipeline-RL(Top_k+Temp) | top_k + temperature | PPO    | 0.8F1 - 0.2延迟   | B1     | 多参数调优效果 |
| R3       | Pipeline-RL(Top_k+Temp) | 同 R2               | GRPO   | F1                | R2     | 比较 PPO/GRPO  |
| R4       | Pipeline-RL(多指标)     | 同 R2               | PPO    | F1+延迟+Token平衡 | R2     | 奖励设计对比   |



### 这样好处

- 架构图直观展示 Pipeline-RL 训练流程
- 对比表让你的论文有结构化的实验分组
- 每一列都能在代码里对应起来
- RL 算法可以从 SB3 调包跑（PPO/DQN）
- 如果你要做生成器级 RL（GRPO/SFT/DPO），可以作为平行实验对照



我建议下一步：

1. 先把 Baseline + R1 跑通（只调 top_k）

2. 再加温度（R2）

3. 最后加 GRPO（R3）

   这样能保证你实验最小可行闭环（MVP）先出来。

